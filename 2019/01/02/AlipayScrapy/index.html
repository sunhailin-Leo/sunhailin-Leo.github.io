<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="支付宝爬虫(Scrapy版本)"><meta name="keywords" content="Scrapy,Alipay"><meta name="author" content="sunhailin-Leo,undefined"><meta name="copyright" content="sunhailin-Leo"><title>支付宝爬虫(Scrapy版本) | LeoBlog</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=undefined"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?19982fade18128291ac156db52a34f03";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: {"labels":{"input_placeholder":"查询文章","hits_empty":"找不到任何记录: ${query}"},"path":"search.xml"}
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Now"><span class="toc-number">1.</span> <span class="toc-text">目前的进度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q&A"><span class="toc-number">2.</span> <span class="toc-text">问题反馈</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DevelopEnv"><span class="toc-number">3.</span> <span class="toc-text">开发环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DevelopEnv"><span class="toc-number">4.</span> <span class="toc-text">安装和运行方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Function"><span class="toc-number">5.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TechPoints"><span class="toc-number">6.</span> <span class="toc-text">技术点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FreeChat"><span class="toc-number">7.</span> <span class="toc-text">题外话</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Future"><span class="toc-number">8.</span> <span class="toc-text">未来的进度</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://s1.ax1x.com/2018/01/28/pvTc0f.jpg"></div><div class="author-info__name text-center">sunhailin-Leo</div><div class="author-info__description text-center">会爬虫会数据分析想学大数据还没毕业的苦逼孩纸</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">13</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">12</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">链接</div><a class="author-info-links__name text-center" href="https://juejin.im/user/5a17ce3451882503eb4af750" target="_blank">我的掘金博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2018/01/28/pvTemV.jpg);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">LeoBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span></div><div id="post-info"><div id="post-title">支付宝爬虫(Scrapy版本)</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Python/">Python</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1,732</span><span class="post-meta__separator">|</span><span>阅读时长: 6 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><hr>
<h3 id="Now">目前的进度</h3>

<ul>
<li><p>2019年1月2日:</p>
<ul>
<li><p>目前建议运行AlipayCore_v2的代码(如果没有装pywin32的依赖的话尽量不要使用AlipayCore)</p>
</li>
<li><p>好久一段时间没有更新了(账单通过selenium的方式基本上都会被支付宝识破了)</p>
</li>
<li>爬取账单的接口的核心就是cookie的有效性,打算之后从cookie的有效性上开始入手</li>
<li>目前代码基本定型,后续需要交流的可以加我的微信或者QQ或者提issue也可以(目前暂时因为工作原因没有太多精力去研究了)</li>
<li><p>可能通过验证码登录的方式会比较有效(但是偶尔也会被封~)</p>
</li>
<li><p>新增的一些东西:</p>
</li>
<li>spiders下新增了一个v2的方式就是通过Scrapy结合urllib的形式进行爬取(偶尔可行,主要还是需要面对支付宝的点击流模型)</li>
<li>utils下就新增bill_page_option(一下账单的参数), bill_parser简单的urllib的爬虫</li>
</ul>
</li>
<li><p>2018年11月30日:</p>
<ul>
<li>今天先整理下代码, 好长一段时间没有认真看这段代码了,这几天会梳理一下也有可能会更新一下代码</li>
</ul>
</li>
</ul>
<ul>
<li>2018年9月16日:<ul>
<li>更新了一下用户主页的数据获取(账户 余额、余额宝详情、花呗详情)</li>
<li>账单详情依旧无法完整获取(账单下载也有点击流识别)<ul>
<li>尝试使用了pywin32和pymouse系列的方式去点击(暂时没有什么进展), 打算用C写个dll真实模拟鼠标移动(待更新)</li>
</ul>
</li>
<li>conf文件夹中放了一个pywin32-221 python3.4的安装包(windows的, 感兴趣的小伙伴可以尝试一下)</li>
</ul>
</li>
</ul>
<ul>
<li>2018年8月8日:<ul>
<li>支付宝又双叒叕改了个人主页的样式和数据获取方式<ul>
<li>之前有一段时间可以通过接口进行获取,但现在支付宝取消了这个接口</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>2018年7月2日:</p>
<ul>
<li><p>题外话:</p>
<ul>
<li>又好一段时间才捡起这个项目.对支付宝这个反爬开始绝望了.</li>
<li>最近应该会研究下…感觉selenium解决不了问题…</li>
</ul>
</li>
<li><p>更新项目:</p>
<ul>
<li>解决了下Windows控制台的报错的问题(加入了一个win_unicode_console包)</li>
<li>删除了一些无用的代码</li>
</ul>
</li>
</ul>
</li>
<li><p>2018年3月14日:</p>
<ul>
<li><p>题外话:</p>
<ul>
<li>好一段时间没有更新了，支付宝的反爬有点强.慢慢研究~</li>
</ul>
</li>
<li><p>更新项目：</p>
<ul>
<li>加入二维码登录的方式（需要自己手动扫码）</li>
</ul>
</li>
</ul>
</li>
<li><p>2018年2月4日:</p>
<ul>
<li>更新说明看: <a href="https://github.com/sunhailin-Leo/AlipaySpider-Scrapy/releases" target="_blank" rel="noopener">Release</a></li>
</ul>
</li>
<li><p>2018年1月:</p>
<ul>
<li><p>将更新提上日程,在测试二维码登录.先上个半成品</p>
</li>
<li><p>原先密码登陆的现在基本上不能用了.因为个人页面多了一种反爬手段,其次就是跳出二维码页面.</p>
</li>
<li><p>上面这些问题,将在之后尽量解决.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>大概在2017年11月~12月的样子：</p>
<ul>
<li><p>开始出现跳出验证码页面了.原因应该是支付宝反爬的模型增强了.</p>
</li>
<li><p>这段时间维护时间不多,都是个人测试没有更新代码上去</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>2017年10月 参加DoraHacks时:</p>
<ul>
<li>当时能够获取到账单和账户信息.</li>
</ul>
</li>
</ul>
<a id="more"></a>
<hr>
<h3 id="Q&A">问题反馈</h3>

<p>在使用中有任何问题，可以反馈给我，以下联系方式跟我交流:  </p>
<ul>
<li><p>Author: Leo</p>
</li>
<li><p>Wechat: Leo-sunhailin</p>
</li>
<li><p>E-mail: 379978424@qq.com </p>
</li>
<li><p>Github URL: <a href="https://github.com/sunhailin-Leo/AlipaySpider-Scrapy" target="_blank" rel="noopener">项目链接</a></p>
</li>
</ul>
<hr>
<h3 id="DevelopEnv">开发环境</h3>

<ul>
<li><p>系统版本：Win10 x64</p>
</li>
<li><p>Python版本：3.4.4</p>
<ul>
<li><p>Python库版本列表:</p>
<ul>
<li><p>Pillow: 5.0.0</p>
</li>
<li><p>Scrapy：1.4.0</p>
</li>
<li><p>selenium：3.8.1</p>
</li>
<li><p>requests：2.18.4</p>
</li>
<li><p>pymongo：3.6.0</p>
</li>
<li><p>python_dateutil：2.6.1</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Ps: 一定要配好Python的环境,不然Scrapy的命令可能会跑不起来</p>
</li>
</ul>
<hr>
<h3 id="DevelopEnv">安装和运行方式</h3><br><em> 安装库<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 项目根目录下,打开命令行</span></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

</em> 启动<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 项目根目录下,启动爬虫</span></span><br><span class="line">scrapy crawl AlipaySpider -a username=<span class="string">"你的用户名"</span> -a password=<span class="string">"你的密码"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 必选参数</span></span><br><span class="line">-a username=&lt;账号&gt;</span><br><span class="line">-a password=&lt;密码&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可选参数</span></span><br><span class="line">-a option=&lt;爬取类型&gt;</span><br><span class="line"><span class="comment"># 1 -&gt; 购物; 2 -&gt; 线下; 3 -&gt; 还款; 4 -&gt; 缴费</span></span><br><span class="line"><span class="comment"># 这里面有四种类型数据对应四种不同的购物清单</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################</span></span><br><span class="line"><span class="comment"># 实验版本</span></span><br><span class="line">scrapy crawl AlipayQR</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂时还没有参数, 能登陆到个人页面了.</span></span><br></pre></td></tr></table></figure><br><br>—<br><br><h3 id="Function">功能</h3>

<ol>
<li>模拟登录支付宝(账号密码和二位都可以登陆)</li>
<li>获取自定义账单记录和花呗剩余额度(2017年10月份的时候个人页面还有花呗总额度的,后面改版没有了.再之后又出现了,应该是支付宝内部在做调整)</li>
<li>数据存储在MongoDB中(暂时存储在MongoDB,后续支持sqlite,json或其他格式的数据)</li>
<li>日志记录系统,启动爬虫后会在项目根目录下创建一个Alipay.log的文件(同时写入文件和输出在控制台)</li>
</ol>
<hr>
<h3 id="TechPoints">技术点</h3>

<p>吐槽一下: 这点可能没啥好说,因为代码是从自己之前写的用非框架的代码搬过来的,搬过来之后主要就是适应Scrapy这个框架,理解框架的意图和执行顺序以及项目的结构,然后进行兼容和测试。</p>
<p>我这个项目主要就用到Spider模块(即爬虫模块),Pipeline和item(即写数据的管道和实体类)</p>
<p>Downloader的那块基本没做处理,因为核心还是在用selenium + webdriver,解析页面用的是Scrapy封装好的Selector.</p>
<p><strong>Scrapy具体的流程看下图: (从官方文档搬过来的)</strong></p>
<p><img src="http://scrapy-chs.readthedocs.io/zh_CN/0.24/_images/scrapy_architecture.png" alt="image"></p>
<hr>
<h3 id="FreeChat">题外话</h3>

<p>题外话模块:<br>    上一段讲到了一个Selector,这个是东西是Scrapy基于lxml开发的,但是真正用的时候其实和lxml的selector有点区别.</p>
<p>举个例子吧：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 两段相同的标签获取下面的文字的方式</span></span><br><span class="line"><span class="comment"># lxml</span></span><br><span class="line">name = str(tr.xpath(<span class="string">'td[@class="name"]/p/a/text()'</span>).strip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scrapy</span></span><br><span class="line">name = tr.xpath(<span class="string">'string(td[@class="name"]/p/a)'</span>).extract()[<span class="number">0</span>].strip()</span><br></pre></td></tr></table></figure></p>
<p>两行代码对同一个标签的文字提取的方法有些不一样,虽然到最后的结果一样。</p>
<p>lxml中有一个”string(.)”方法也是为了提取文字,但是这个方法是要在先指定了父节点或最小子节点后再使用,就可以获取父节点以下的所有文字或最小子节点对应的文字信息.</p>
<p>而Scrapy的Selector则可以在”string(.)”里面写入标签,方便定位,也很清晰的看出是要去获取文字信息.</p>
<p>具体区别其实可以对比下我非框架下的和Scrapy框架下的代码,里面用xpath定位的方式有点不一样.</p>
<ol>
<li>selenium + lxml: <a href="https://github.com/sunhailin-Leo/Alipay-Spider" target="_blank" rel="noopener">非框架</a></li>
<li>Scrapy + selenium: <a href="https://github.com/sunhailin-Leo/AlipaySpider-Scrapy" target="_blank" rel="noopener">Scrapy</a></li>
</ol>
<hr>
<h3 id="Future">未来的进度</h3>

<ol>
<li>数据源保存的可选择性(从多源选择单源写入到多源写入)</li>
<li>修改配置文件的自由度(增加修改settings.py的参数)</li>
<li>尽可能优化爬虫的爬取速度</li>
<li>研究Scrapy的自定义命令的写法,提高扩展性</li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">sunhailin-Leo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.leoyuki.xyz/2019/01/02/AlipayScrapy/">http://www.leoyuki.xyz/2019/01/02/AlipayScrapy/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明来自 <a href="http://www.leoyuki.xyz" target="_blank">LeoBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Scrapy/">Scrapy</a><a class="post-meta__tags" href="/tags/Alipay/">Alipay</a></div><elif theme.sharejs && theme.sharejs.enable></elif></article><nav id="pagination"><div class="next-post pull-right"><a href="/2018/08/31/summary_2018_8/"><span>2018年8月总结</span><i class="fa fa-chevron-right"></i></a></div></nav><elif theme.laibili && theme.laibili.enable></elif><elif theme.gitment && theme.gitment.enable></elif><elif theme.valine && theme.valine.enable><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
    return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
    el:'#vcomment',
    notify:notify,
    verify:verify,
    appId:'PUKLIKgjUz2sCajRl5gDbQQC-gzGzoHsz',
    appKey:'xoXTBzM4M358QIjKygcSRsXi',
    placeholder:'Just go go',
    avatar:'mm',
    guest_info:guest_info,
    pageSize:'10'
})</script></elif></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2019 By sunhailin-Leo</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="icp"><a href="http://www.miitbeian.gov.cn/"><span>粤ICP备18015921号-1</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.4.2"></script><script src="/js/fancybox.js?version=1.4.2"></script><script src="/js/sidebar.js?version=1.4.2"></script><script src="/js/copy.js?version=1.4.2"></script><script src="/js/fireworks.js?version=1.4.2"></script><script src="/js/transition.js?version=1.4.2"></script><script src="/js/scroll.js?version=1.4.2"></script><script src="/js/head.js?version=1.4.2"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>